{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomic sequence classification with deep learning\n",
    "\n",
    "## Outline:\n",
    "1. Objectives\n",
    "2. What is *deep learning*?\n",
    "3. Why the sequence classification problem is important?\n",
    "4. Practice\n",
    "    * Dataset\n",
    "    * Models\n",
    "    * Training\n",
    "    * Evaluation\n",
    "5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "1. Learn the basic theory and practice of deep learning\n",
    "2. Understand the basic deep learning workflow\n",
    "3. Deploy a model for genomic sequence classification\n",
    "    * With helpers\n",
    "    * Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is *Deep Learning*?\n",
    "Deep Learning is a subset of machine learning techniques that uses **artificial neural network**-based models (ANN). What makes it **deep** is the presence of many transformation *layers* within the models. Figure 1 shows a Venn diagram of the organization between artificial intelligence, machine and deep learning. Figure 2 shows a representation of a Multilayer Perceptron (MLP), the most basic architecture in deep learning, which mimics the biological neural conections.\n",
    "\n",
    "Deep learning models are able to learn from raw data. This is one of the main differences against traditional learning pipelines. With these kind of models you can assemble a learning system that tunes itself automatically rather than fixing and updating each individial component one by one. It replaces some of the labor-intensive processes needed for other methods, like field-specific data preprocessing and/or manual feature extraction. Deep learning models can learn and process these features in an automated fashion, generate accurate predictions, and be fine-tuned for specific applications when an available model exists.\n",
    "\n",
    "One of the most common and persistent disadvantages of applying deep learning methods in your work is the large amount of data needed to train the model. To capture the features and generalize the phenomena in your studies, a substantial amount of data (sometimes labeled) must be available so the model can fit them and produce accurate predictions. However, with the ever-increasing availability of graphical processing units (GPUs), the massive amounts of data generated in clinical and biological scenarios, and the possibility of fine-tuning existing models, implementing a deep learning architecture for your specific applications is becoming increasingly simple.\n",
    "\n",
    "<fig>\n",
    "<img    src=\"images/ENG_IA_ML_DL.png\"\n",
    "        width=600\n",
    "        height=600>\n",
    "<figcaption>Fig. 1: Venn diagram displaying the organization between AI, machine, and deep learning.\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<fig>\n",
    "<img    src=\"images/MLP.png\"\n",
    "        width=600\n",
    "        height=600>\n",
    "<figcaption>Fig. 2: Grapphical representation of an MLP.\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code is used to import the necessary libraries for the notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from genomic_benchmarks.data_check import list_datasets, info, is_downloaded\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersCohn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the **Genomic Benchmarks** datasets, a set of benchmarks for classification of genomic sequences to test models' capabilities.\n",
    "\n",
    "In the next code cell we list the available datasets in the **Genomic Benchmarks** module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demo_coding_vs_intergenomic_seqs',\n",
       " 'human_enhancers_ensembl',\n",
       " 'human_ocr_ensembl',\n",
       " 'human_enhancers_cohn',\n",
       " 'human_nontata_promoters',\n",
       " 'dummy_mouse_enhancers_ensembl',\n",
       " 'drosophila_enhancers_stark',\n",
       " 'demo_human_or_worm',\n",
       " 'human_ensembl_regulatory']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative pourpuses you will work on the `human_enhancers_cohn` which contains multiple genomic sequences that are or are not enhancers for Cohn disease. In machine learning verbose, you will work on a **binary classification** problem. We can display some information of this dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `human_enhancers_cohn` has 2 classes: negative, positive.\n",
      "\n",
      "All lengths of genomic intervals equals 500.\n",
      "\n",
      "Totally 27791 sequences have been found, 20843 for training and 6948 for testing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>10422</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>10421</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train  test\n",
       "negative  10422  3474\n",
       "positive  10421  3474"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(\"human_enhancers_cohn\", version=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `genomic_benchmarks` module offers multiple data handlers and helpers to load, show and give you an idea of how each of its datasets are composed.\n",
    "In the next code cell we use the `HumanEnhancersCohn` function to download and assign the dataset into two variables, `train_dataset` and `test_dataset`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and split it into training and test sets\n",
    "train_dataset = HumanEnhancersCohn(split=\"train\", version=0)\n",
    "test_dataset = HumanEnhancersCohn(split=\"test\", version=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure we imported the correct dataset we can print the lengths of each set and check if the numbers match the ones shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training dataset: 20843. Length of the test dataset: 6948\n"
     ]
    }
   ],
   "source": [
    "# Print the lengths of each set check if they match the info we saw previously\n",
    "print(f\"Length of the training dataset: {len(train_dataset)}. Length of the test dataset: {len(test_dataset)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how this data actually look like? You have only downloaded some data from a library for binary classification up to this point. How can you actually see some samples? Turns out to be very easy to do so! Let's see two examples, one for the `positive` and one for the `negative` dianogses, correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AGCAGCAGGTCAACATTTTTGCACTCACAAAATAATTTGGAAAAACTATATACCTCTTTCACATTTTTTTTTTTTTGAGATGGAGTCTCACTCTGTCGCCCAGGCTGGAGTGCAGTGGTGCAATCTCGGCTCACTGCAAGCTCTGACTCCTGGATTCATGCCATTCGCCTGCCTCAGCCTCCCGAGTAGCTGGGACTATAGGCGCCCGCCACCATGCCTGGCTAATTTTTTGTATTTTTAGTAGAGACGGGGTTTCCCCGTGTTAGCCAGGACGGTCTCTAGCTCCTGACCTTGCGATCCACCTGCCTCGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCACTGCACCAGGCCCTCTTTCACATTTTTAAGTTTTCTGTTATCTATTTCAAAAGGTGTAGTTAACATATTTTAAATATTAACAATTCAAAAATAAAACTATTATAGAATTTTTAAACAGTATCCAGATAAATTTTTATTATTAATTTCATACTCAA', 1)\n",
      "('CTGATGAAACCCGGCGAGGTGTGGTCTGCCCTGGAGGACAGCAGCCAGTGTGGGGGGCAGTCCCTTCTCTCTGTCCCAAGGGAGGATACAGCTCCACTGTGGTCACTGGCTCTATGTGAGGGGGTGCATGCATCAGAGACAACAGATGAGAGGGCCCTTCAGTTGGCTTTTCTGCCTCCAGTTCTTTCTGTTCATGAGAGGAAAAGCTACTGGTAGACAGAACAATGTTAAATGTAATAAAAATAAGCAAGTTCCCTGGGTTTATGCAGTGCCAAATGTCAAGATGGTTGTATACAGGAGAAGACGTCCAAGACACGTCTTTTCCGAGTGTCCCAGAGCTCAGAACTCTGTGAGCACTTTGAGCTTCCCCAGACCTCTTTCTTCCCTGGGTGTGAGCCCTGCACAGTGCTCCGAAAAGAGCTGGGGTCCGTAAATACGGATGGCAAACAGCTCACCTGGGTTTCTCACATGGATTTGTTTTCTTGGGGGTCTCTGTATGG', 0)\n"
     ]
    }
   ],
   "source": [
    "# Get a sample from train_dataset with a 1 on the second element of the tuple\n",
    "positive_sample = next(filter(lambda x: x[1] == 1, train_dataset)) # A 1 indicates that the sample is a positive sample\n",
    "negative_sample = next(filter(lambda x: x[1] == 0, train_dataset)) # A 0 indicates that the sample is a negative sample\n",
    "print(f\"{positive_sample}\\n{negative_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell, without looking at the corresponding labels, which of these sequences is positive for an enhancer of Cohn's disease and which is not?\n",
    "Well, you can determine this by some other studies like genome-wide chromatin immunoprecipitation or RNA sequencing. But if you saw these sequences alone, would you be able to tell whether it is an enhacer or not? A deep learning model can do it! At the cost of massive amounts of already-labeled data that were obtained with techniques like the previously mentioned ones (that is, what we just downloaded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already have downloaded the dataset, but it isn't ready to be used with a neural network yet. PyTorch neural networks expect their inputs to be arranged in a special data structure known as **tensor**s. If you have experience with Numpy's `ndarray`s getting to know tensors will be pretty easy. These are n-dimensional number arrays optimized for gradient calculus and other operations that run in the background when training a neural network. More information about tensors and specifications on [Pytorch's website](https://pytorch.org).\n",
    "\n",
    "Now, how does a tensor look like? In the following code cells we initialize random tensors and display them just for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.tensor([1, 2, 3, 4, 5]) # As you can see, it takes a list an input\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor's data type is an important consideration always. Many errors arise when tensor's data types aren't the same bewteen the inputs and the labels. Specifically, this error arises when calculating the **loss**, a value that measures the difference between the model predictions and the ground-truth (your labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor.dtype # Using the dtype attribute, we can see the data type of the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manipulate tensors by adding or removing elements, changing their data types, do any mathematical or arithmetical operation with them, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "my_tensor = my_tensor.to(torch.float64) # We can change the data type of the tensor using the .float() method\n",
    "print(my_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  4.,  9., 16., 25.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tensor_mul = my_tensor * my_tensor # We can perform element-wise multiplication on tensors\n",
    "print(tensor_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know you need these sequences in a specific format, the next question is: how do you transform these sequences into representable tensors? The answer lies in the **Natural Language Processing** (NLP) field. NLP is a sub-field in computer science and AI that uses different kinds of algorithms to enable computers to understand human spoken language. Its applications range from text encoding and generation, voice recognition, and speaking systems. Some examples in daily life are chatbots, comand execution through voice activation (Amazon's Alexa), digital assistants (Bixby, Siri, Google Assistant), etc.\n",
    "\n",
    "In this case we are working on sequence classification, this means each of these sequences have an inherent \"grammar\" or structure. As in NLP they analyze sentence grammatics and decompose them by encoding each word you're going to do the same. The process of encoding words and turning them into meaningful numerical representations is called **tokenization**.\n",
    "\n",
    "Nowadays it is really easy to implement and use a tokenizer to convert our raw genomic sequences into representable numbers. Thanks to HuggingFace's `transformers` and `tokenizers` library, you can download and use pretrained neural networks and their corresponding tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tokenizer\n",
    "checkpoint = 'LongSafari/hyenadna-tiny-1k-seqlen-hf' # This is the model's name we are going to use\n",
    "max_length = 1024 # This variable will represent the maximum length of the input sequences\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this tokenizer to the first sequence to look how the data comes out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [8, 10, 9, 7, 10, 9, 7, 7, 7, 8, 8, 8, 9, 9, 8, 9, 7, 9, 9, 10, 9, 10, 9, 9, 10, 8, 10, 9, 8, 8, 8, 10, 9, 9, 7, 9, 9, 7, 8, 7, 9, 8, 7, 9, 8, 8, 7, 9, 10, 9, 10, 9, 9, 9, 9, 9, 9, 8, 7, 9, 10, 8, 8, 8, 10, 10, 8, 10, 8, 10, 8, 10, 9, 10, 8, 8, 8, 7, 7, 9, 9, 9, 7, 9, 9, 7, 10, 7, 8, 7, 9, 8, 10, 8, 8, 7, 8, 10, 9, 10, 9, 9, 10, 8, 7, 8, 10, 9, 9, 8, 10, 8, 10, 7, 10, 9, 10, 9, 7, 9, 9, 9, 9, 9, 10, 9, 8, 7, 10, 9, 8, 7, 10, 8, 7, 9, 7, 9, 7, 8, 7, 7, 8, 7, 9, 7, 10, 9, 7, 9, 7, 9, 9, 9, 8, 8, 8, 10, 10, 8, 7, 9, 10, 10, 9, 9, 8, 10, 10, 10, 10, 8, 10, 9, 8, 8, 10, 8, 8, 7, 9, 10, 10, 8, 10, 10, 10, 8, 10, 9, 10, 10, 8, 7, 10, 9, 7, 9, 7, 9, 9, 7, 7, 7, 7, 9, 8, 10, 7, 8, 10, 9, 9, 10, 7, 9, 7, 8, 7, 9, 7, 7, 8, 7, 7, 10, 9, 10, 10, 7, 7, 7, 10, 9, 10, 7, 7, 10, 7, 7, 7, 7, 7, 10, 7, 7, 9, 8, 7, 7, 9, 10, 10, 8, 8, 8, 10, 9, 9, 9, 10, 10, 10, 7, 10, 9, 8, 7, 9, 10, 9, 8, 8, 7, 7, 7, 10, 9, 10, 8, 7, 7, 9, 7, 10, 9, 9, 10, 10, 9, 10, 7, 10, 7, 8, 7, 9, 9, 7, 9, 7, 7, 9, 7, 8, 9, 10, 8, 8, 7, 7, 9, 7, 8, 7, 8, 9, 10, 8, 10, 10, 10, 10, 8, 8, 9, 7, 9, 10, 9, 10, 8, 8, 8, 7, 9, 7, 9, 8, 10, 8, 7, 9, 7, 7, 8, 10, 8, 10, 9, 10, 9, 7, 9, 8, 7, 8, 10, 10, 10, 9, 7, 9, 8, 10, 10, 8, 8, 8, 8, 7, 9, 7, 8, 8, 10, 8, 10, 10, 10, 8, 10, 10, 8, 8, 8, 10, 9, 9, 9, 10, 9, 10, 9, 7, 9, 8, 8, 8, 10, 9, 8, 7, 8, 7, 9, 10, 9, 8, 10, 8, 8, 9, 7, 7, 7, 7, 9, 7, 9, 8, 10, 9, 9, 9, 9, 10, 8, 8, 9, 10, 7, 7, 7, 10, 7, 8, 9, 9, 7, 10, 9, 9, 8, 7, 7, 7, 8, 7, 9, 8, 10, 8, 7, 8, 8, 10, 9, 9, 9, 10, 10, 10, 8, 10, 8, 7, 8, 7, 10, 9, 9, 7, 10, 10, 10, 9, 10, 10, 10, 10, 8, 10, 10, 9, 9, 9, 9, 9, 10, 8, 10, 8, 10, 9, 10, 7, 10, 9, 9, 1]}\n"
     ]
    }
   ],
   "source": [
    "enconded_sequence = tokenizer(train_dataset[0][0])\n",
    "print(enconded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now your sequences is represented as a bunch of numbers. Each of them represents each of the nucleotides that conform the sequences we used as input and the tokenizer adds some special tokens to differentiate from sequence to sequence when passed into the training phase. Let's see how the network will read your sequences in meaningful way for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTGATGAAACCCGGCGAGGTGTGGTCTGCCCTGGAGGACAGCAGCCAGTGTGGGGGGCAGTCCCTTCTCTCTGTCCCAAGGGAGGATACAGCTCCACTGTGGTCACTGGCTCTATGTGAGGGGGTGCATGCATCAGAGACAACAGATGAGAGGGCCCTTCAGTTGGCTTTTCTGCCTCCAGTTCTTTCTGTTCATGAGAGGAAAAGCTACTGGTAGACAGAACAATGTTAAATGTAATAAAAATAAGCAAGTTCCCTGGGTTTATGCAGTGCCAAATGTCAAGATGGTTGTATACAGGAGAAGACGTCCAAGACACGTCTTTTCCGAGTGTCCCAGAGCTCAGAACTCTGTGAGCACTTTGAGCTTCCCCAGACCTCTTTCTTCCCTGGGTGTGAGCCCTGCACAGTGCTCCGAAAAGAGCTGGGGTCCGTAAATACGGATGGCAAACAGCTCACCTGGGTTTCTCACATGGATTTGTTTTCTTGGGGGTCTCTGTATGG[SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(enconded_sequence['input_ids'])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to obtain the PyTorch tensors directly just add the `return_tensors=\"pt\"` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 8, 10,  9,  7, 10,  9,  7,  7,  7,  8,  8,  8,  9,  9,  8,  9,  7,  9,\n",
      "          9, 10,  9, 10,  9,  9, 10,  8, 10,  9,  8,  8,  8, 10,  9,  9,  7,  9,\n",
      "          9,  7,  8,  7,  9,  8,  7,  9,  8,  8,  7,  9, 10,  9, 10,  9,  9,  9,\n",
      "          9,  9,  9,  8,  7,  9, 10,  8,  8,  8, 10, 10,  8, 10,  8, 10,  8, 10,\n",
      "          9, 10,  8,  8,  8,  7,  7,  9,  9,  9,  7,  9,  9,  7, 10,  7,  8,  7,\n",
      "          9,  8, 10,  8,  8,  7,  8, 10,  9, 10,  9,  9, 10,  8,  7,  8, 10,  9,\n",
      "          9,  8, 10,  8, 10,  7, 10,  9, 10,  9,  7,  9,  9,  9,  9,  9, 10,  9,\n",
      "          8,  7, 10,  9,  8,  7, 10,  8,  7,  9,  7,  9,  7,  8,  7,  7,  8,  7,\n",
      "          9,  7, 10,  9,  7,  9,  7,  9,  9,  9,  8,  8,  8, 10, 10,  8,  7,  9,\n",
      "         10, 10,  9,  9,  8, 10, 10, 10, 10,  8, 10,  9,  8,  8, 10,  8,  8,  7,\n",
      "          9, 10, 10,  8, 10, 10, 10,  8, 10,  9, 10, 10,  8,  7, 10,  9,  7,  9,\n",
      "          7,  9,  9,  7,  7,  7,  7,  9,  8, 10,  7,  8, 10,  9,  9, 10,  7,  9,\n",
      "          7,  8,  7,  9,  7,  7,  8,  7,  7, 10,  9, 10, 10,  7,  7,  7, 10,  9,\n",
      "         10,  7,  7, 10,  7,  7,  7,  7,  7, 10,  7,  7,  9,  8,  7,  7,  9, 10,\n",
      "         10,  8,  8,  8, 10,  9,  9,  9, 10, 10, 10,  7, 10,  9,  8,  7,  9, 10,\n",
      "          9,  8,  8,  7,  7,  7, 10,  9, 10,  8,  7,  7,  9,  7, 10,  9,  9, 10,\n",
      "         10,  9, 10,  7, 10,  7,  8,  7,  9,  9,  7,  9,  7,  7,  9,  7,  8,  9,\n",
      "         10,  8,  8,  7,  7,  9,  7,  8,  7,  8,  9, 10,  8, 10, 10, 10, 10,  8,\n",
      "          8,  9,  7,  9, 10,  9, 10,  8,  8,  8,  7,  9,  7,  9,  8, 10,  8,  7,\n",
      "          9,  7,  7,  8, 10,  8, 10,  9, 10,  9,  7,  9,  8,  7,  8, 10, 10, 10,\n",
      "          9,  7,  9,  8, 10, 10,  8,  8,  8,  8,  7,  9,  7,  8,  8, 10,  8, 10,\n",
      "         10, 10,  8, 10, 10,  8,  8,  8, 10,  9,  9,  9, 10,  9, 10,  9,  7,  9,\n",
      "          8,  8,  8, 10,  9,  8,  7,  8,  7,  9, 10,  9,  8, 10,  8,  8,  9,  7,\n",
      "          7,  7,  7,  9,  7,  9,  8, 10,  9,  9,  9,  9, 10,  8,  8,  9, 10,  7,\n",
      "          7,  7, 10,  7,  8,  9,  9,  7, 10,  9,  9,  8,  7,  7,  7,  8,  7,  9,\n",
      "          8, 10,  8,  7,  8,  8, 10,  9,  9,  9, 10, 10, 10,  8, 10,  8,  7,  8,\n",
      "          7, 10,  9,  9,  7, 10, 10, 10,  9, 10, 10, 10, 10,  8, 10, 10,  9,  9,\n",
      "          9,  9,  9, 10,  8, 10,  8, 10,  9, 10,  7, 10,  9,  9,  1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_tensors = tokenizer(train_dataset[0][0], return_tensors='pt')\n",
    "print(encoded_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output's data structure is a dictionary containing the `input_ids` key which contains the actual tensors you're going to use to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `genomic_benchmarks` datasets contain their labels already in a tensor format. If we load these dataset in another pytorch-specific data structure called `DataLoader` we can display mini-batches of samples from the whole dataset. This looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 0\n",
      "Inputs: ('TGAAGTAGAGGGACCTATGAAATCATTGGTTGCAAAAAATGGAGGTAAGCTAGTAGCCAAAGTATCTTATAGAATTAATACTATAGGCACTTTGTAAAATATTTATTTACAGAGTGCACTGAGAAATTAACTGTGAGAATGATTAAGCAATCTGTTTAAGATTATAGAGAGAGACAAAGAGAAAAACAAAAAGTTCTTTGAATTTTTTTTTTTTTTTTTTTTTTTGAGACAGAGTCTCACTGTGTTGCCCAGGCTGGAGTGCAGTGGCACGACCTCGGCTCACTGCAAGCTCTGCCTCCCGGGTTCACACCATTTTCCTGCCTCAGCCTCCCGAGTAGCTGGGACCACAGGCGCCCAGCACCAGGCCAGCTAATTTTTTGTATTTTTAGTAGAGACAGGGTTTCACCGTGTTAGCCAGGATGGTCTCCATCTCCTGAACTTGTGATCCGCCCTCCTCGGCCTCCCAAAGTGCTGGGATTACAGGTGTGAGCCACTCCGCC', 'GCTGGCATTCTAGGAGGTAGATCCACTCGAAAGCAATCAAAGCCACGGAGTGTGTGATTCTGACAGAGGGGGTGGTGGGATGTCAGCGGTGGAGGGCTTCTTATGGCCTAGAAAAGTGAGGGGACTTCTTGCAGAAGTGGGCTTGACCTGGGCTTTTCAGGATGGGAGCAAATTTGAAGGAGGAGAGAGGATACCTCCATGTGCTCACACATTTGCACACACATTTACTCTCGTGCACACACCCAGTCCCTTAAGCCCCCCATCTATAGGTGCTCACAGCCAGCAGTATGCACACACACCTCACACACAGCCAAAGTAATGCACACAAACTCAGAAACACACACACTGGGGCATTCACACTCTTATGAGGCATGCCGATGCTACAACGGTCGCCCACATACACTCTGGGCACCAGAAACATACAGGCCCCTTAGAAATGGGCTTGGGAGAAGATGAAGATGTGTCTGCAATGCGTAAGAAGAGCAGAAGCTGACAGGTGG', 'GTCTTCACCTCCTCTCAGACACCCCATGGAAACAACAACACAGAGGCATGCCTTGGGGCCAGCCTTGTCTCATGTCTGGAGGAATTCCTTCCTGTAGAGAGTCCCACTTCTCAGGTCAGTTTCCGTGGCTCGGCATGTGGTGGTCCAGCCTTGCTTCCACATCATTCCTCACGTTCCTTTGTGGCTTAACAAGAGCGTCCCATCTCCCGCCCCTCCATCCTGGTGGCACTCAGAAGGTTCTGTTTTGAGTTCCAGCGGCAGTTACTCCCTTTTGGGGAAGGCCTGTTGGCCGGGGAGGCGGCATGGGGTGGGGGCTGGCCAGGTTGGCATGTGCTCCCTGTGCATTTGTCTCTCGAGGTGGTGTATCGAAGATCTTTGTTTTTAATCATAATATCTCACATATTTTCTTTCTGTGCTGCTCAGTGAAAGCTAGCTAGGGGTGGCGGCAAATCTTCCTTTAATTTCCACTGTGATGGATTATTAGTTCACTTAAAATGGTG', 'TTGAGTAAGTATGGACACGAAGAAGGAAATGACACTCACCAGGGCCTACTTGAAGGTGAGGGTGGAAGAAGAGTGAGGATCAAAAAACTACCTATCGGTACTATGCTTATTACCTGATTGATGAAATAATCGGTACACCAAAGCACCGAGACACGAAATTTACCTATATAAGAAACCTATACATATACCCCTGAACCTAAAATAAAAATTAATATAAGTAAAAAACAATACAGTAAAAAAGATTTACAAACATTTACATTGTATTATGTGTCATGAATAAGCTAAAGATCATTTAAGTATAGAAGATGTATATAGGTTATATGTAAATACTGTGCCATTTTACACTGAGGACTTGAGTATATTTAAATTTTGTTATCTGTGGGGGTCCTGACACAAATCCCCCATGGATTCTGAGGGATGACTGTATATAATATCACAAGTACATGCAAAGATGTTCAATGACATTAGTCTTAAGGAAAACGCAAGTCAAAATTACAGGA', 'AAGGTGATGGCAATGATGGCCTGGCTGGTGGGCTCAATGATGAGCAGGGAGGTCCAGAGTCTGATGAAAGCAAGGAATCCTCCAAAGGCCTCCAGGATATAGGCATAGCTGGCCCCAGATTTCTTAATGGTGGTGCCCAGTTCCGCATAACAAAGGGCCCCAAAGACGGAGAAGAGGCCCCCGACAGCCCAGATGACCAGAGAGAGACCAAAGGAGGCACTGTATATGAGCACACCCTTGGGGGAAACAAAGATGCCCGAGCCGATCATGTTCCCCACAATCAGGCACACGCCGTTAAGCAGTGAGATCTCCTTCTTCAGCTTCACCTGCTCCGGCCCTGGGCTGGCCCCATCACCCAAAGGGGAGGTTTCCACCTCAGGCTGGGAGGCCACTTCATACTCAGTGCTGTCAACCATGGTGGAGGAGAGGAAACCCTTCACCAGCTTCCTGGCATTGCCCTTTAAGGAAGAAAGATGATGCTATAGATTAGGTGGTTGGCA', 'CCCAATACCTACCAGCTGCTCCTTGCTTTAGTTTTAGAATATCCCTTTTGACCTTCTCTCTCCCTTTTTGTCACTGTCGTTTGACCCTGAGTTTCCCCTGCCCACATCCTGCCTACAGTGCTTCATGTTGCTTTAAATGACTTCACAGCCCCTTGCTCCTTTAAGTGATCTAAGCTGAGCTGGGTTAGCAGGGCCACTTTCTGTGTAGCAAGGGGACGTCTAAAGGCTGCAGCCCAGCTTGGTTAGCTGCTCTAAGGGGACTCCAGTCTTCCTCACCCCACCAGGTGCCCCAAAGAGCAAAACCAATGTTTCTTTTCATCCATCACCTTTTCCTCAACAACACCAGCTAACATTTGTTAAGCACTTACTCTCTTTCTGGCCCATGTGTTTATGAGCCTAATCCTGACTGCAGCCCTGTGTGGTAGGTGCTTCAGTAAGCCCCATTTGTTAGATAAGAAAATTGACTCCTGGCCGAGGTGCCCCATATGGAAGGTGGTAGA', 'TAGTAGAAATAAAAAGAGAGAAGATGTGTTAAACGGCAACATGGGGATGCAATCATCCAAGTCTTAATTATGGGGACTTTAGCACAAACCATCAGTTGCTTCAACAGAAAAACTGCAATGAGAAAAAAGGAAGAAAAATAAAGCAGATCCTATGGACTAAAAGGATTTAAGTAACATGATTAAACAATAACAATATATAACCTTATGTTGATTCTCTTCAAGCAAACAAAGCATTAAAAATATATAAGAAAATTGAGAAAACGTGAATACAGACTGGATGTTTGATGGTATCAAAGATATTTCTTATTTTTAGGTGTAAAAATTGTGATTGTGTTTTTTATAAAGGATCTTTAGCTTTCATAGATACATAATTAAATATTTACAGATATGATACCTGAGATTTGCTTCGAAATTTTTCTAAGTAAAAGGATGGGGTTCTAGAGGGTCTGGATATAAATGAGACAGAATTGACCATATGTTGATAATTGTGGAACATAAGA', 'GTTAAATTTGATGTTTCCTCTCCCGGTAATGTTTGAGAAAAATATTACCTGGTTAATGCAATCTTTATCTAGCAAAACAGGGGTGAGCTGGTCTAGAATCAATGAGACCAAGGTCTTGAGTTCAAATTGTGCGTGAGTCAGTGAACCTCTCTCTAAAACAGGGCCAAGCCCTCCTCCTAAACCGGCAAATGCACATGGTAGGTCATGGGGGGAGTGGCGAGAGAGCACACACGTGCATAGCACACACCCAGCACTGCCCTGGAGAACTCGAAATCAAACTGCACATTCTGTTGATGTTGTGTCAATGGACTCTCCCCATAAATAGTATTTGGCTCTCAGAGTCAGATAATTTAAATTTCTCCTATTCCTCTCACCACCCTCCCCTCAACTCCCACCTTTTACATTTCTATGTTCTTCCCCACGTAAGCCCCTTACCCTTTTAAAAATCTTTGATAGGGTGCATACATGGAAAAGGGGGTGGATTACTTATTTTAATGATT', 'CTTGCTACACTTCGATTTTCATGTGTAAAATGGGGAGGACAATGCTGAGTTCACATGGTTGCCTCATCTAATTGAATACCTTAGACATTATTGAAGATGAAAGAACCTGTGAGGAGTTGGGTTGTTTTTTTAACTTATTTAATTTTTTTTTTGGACACAGGGTCTTGCTTTGTCAACCAGGCTGGAGTACAATAAGAGTGATCATAGCTCACTGCAGCCTCCACCTTCCAGGCTCAGGTGATCCTCCCACCTCAGCCTCCCGAGTAGCTGGGACCACAAGCAGGTGCCACCATGCCGGACTAATTTGTTTATTTTTAGTAGAGATGGGGTCTTGGTATGTTGCCCAGGCTGGTCTCGAACTCTTGGGCTCAAATGATCCTCCCACCTCAGCGTCAGAAAGTACTGGGGTTATAGACATGAGCCATCACGCCCAGCCTAGGATCTCTCATTATGATTAGTCCATTGCCTTGTTCAGCATGACCATGTCCCTGGGTTGCGTG', 'TGCCTAAACTACCTAGGGGGAGTTTCATTGTCTGAAATCAAGAACTCTGAAAGATTCATGTAGCCTAAAAATAAATTTTGGGTTGCCTTCAAGTTATGACATGCTTCTAATCCTAAATCTGCTTGCTTATGTGAAACAGATTCCATTAATTTTTATACTATATCCTAGAACCATAGAATTCTTGAATTTAGATGTAGAAGTGTCCTTTTAGAGCCCATTTTATTTTATCATTCACCTTCGGCTTTGAGCGAATTTTCATATCACAACCATTTGAGGGAACCAAGTTTCTCACAAAACCAGAAAAAAGGTAATTTATGTGGGCTGAAATGTCTTTGTGCCAGTCATGAGTAGGGCTGAGAGGACACCCTTGGGTAATGTTATTTACGTGAGAGACAGGTGAAGGGCCTGCAGGATGTAACCTAATTTAGTCAGACATTTCTGGCACAAAAAAACCAGTGACCTATAAACCAGTACTTTCCTGCAATTTCTAGTGAAACCAG', 'AAAAAAAAGTAACCTTTCTATTGCCTCTTTGGGTGTTTTTTATAATTATTTTCAGCATATTGGCCCCTTCTTAAGGGGAAATCATGTAGAGTAGAAAAAAAAAAAGCGCACCGGGTCACTGTGATGCCTCCCCAGTTACCACCCGAGTGGCTCCAGCAGACACTCTGGAGACCAGAGCCCGGCCACCAGAGCACCACAGGGGACCGCCACCTGTCAGGGCTTTAAAAATACTGCGTTCCTTTCTCTGTTTCCTTCTACTCTACTTTCACTCTTTTTGAAAGTGAGGTTGGCCTTTTTACCCAAAAACTCTCAAAGTCCTCCCACACACTGTCGTTGGCCACTGTCATTTAGCAGGAGGGAGTGTGTCCAGCCTGGGAAGTGAGCGACTGCAACTCTGTGTTTGCCTAAACACAGTTTCTTGTTGCCGATGCCTGCTCTCTTCTCTTATTTACTCTGTGTACCGAGGGAGGTGGGCAGGCGGTGTGAGTAACCTCTCTGCT', 'TGTTCTGAAAAAAGGAGTAAGTAATTAATAAGAGTATAGACGATGCCTCTATTTTTCACTATGACACCGGCAGAAGAATGTAATAGTATTTGAGAGGATGATGTGAGGACAAAGAAAGGTTTTGTTTTAGGTGATTAGTCTTTCAGCTAGGAGACACTTTAAGCTGAGAGATACTTTAAGCTGGGAGATTTTAGTAACAGGAATTCTAGAGAAGAAAAAAATGTAGCTCAGTGAAGAACTAACTGAAGAACTTGAAGAGTTGAGAAAGTGAATCAAAGAACAAATGAAGGACATGTTTTACGGAAGGGACATCAATTGCATTATAAGAGACGTTATGATGGAGAAGATAGAAGCAGAAAAACGATTTTAAGATTTGGTAATGGGAATATCATATCTCCAGTTACAAGTAACAGAAAACACAATTCAGTGGTTGTGGGAGTTAGTTTTATGTGTCAACTTGACTGAAACAGGGGATTTCCAAATATCCGGTTAAATGTTAT', 'AAAGCCAGGCGCAGGGTGTGGGCCCCTCACTGCAGGGCAATGGCCACTGCACACCGGGGCGGGGCCAGTATGTGACATGTGCTGCCACCCCCTCCTTCAACAGCGTCAACCTCACTAAGCAGCCCAGAACTCCGGGTGAGACAGGTCCCTGAACTGTGGCTGCCCCAGGGTCGTTTCTAAGTGCCCTCCCTGATCCCAGCCCAGCGGGCACACATCTCACCTTAGGATCTCTGCAGATACAGACACAACGGAGCTCTTTTTCCCTATAGTGTTCCTCCTGCTCGGAACCGTCCTTTCCAAACAATCATGTGGGAATACACAGGACTGCCAAGGACTTCACAAACATTTCCCCAAAACATAAACAGTGTTCAGCACTTGGGAGTTTTCAGCTTGGTGGTTTTTCTCCCACCAATAACAAGATGTGGCGCCTCACCAGGCAGTCCCTCCTGAACAGTCTTCAGAGAACCCCTTCACAAAGACCCGAGCTGTCCTTCTGGCCT', 'GCAGGGGCATGATCTTGGCTCACTGCAACCTTGCTTCCCAAGTTCAAGTGATTCTCCTGCCTCAGCCTCCTGAATAGCTGAGATTACAGGCATATGCCATCACGCCTGGCTAATTTTTGTATTTTTAATAGAGATAGGGTTTCAGCATGTTGGACAGGCTGGTCTTCTGACTTCGACTCCTGACTTCAAGTGATCTGCCCACTTCAGCCTCCCAAAGTGCTGGGATTATAGACGTGAGCCACCGTATCCAGCTTCAAAGCCACATACATTTGCAAACAACACTTTATAATTACCAGAGCCTCATCATGAGCTCTGGGAACTTGGGCTGGAGGTCCAGGGCTCAGGACAATAAGAGCTCCTCAATTCTGATGCAGATCAGACTGAGCTCTGGTCTTCTGGTCCACCAGCCTAAAAGCCCTTACTTAATTGGACACAATACCAAAGAAACTCAAAGGATGTGATAGTCACCTGGTATGAAAACATCATGAACAACTGTAAAA', 'GAGAAAAGGCCTGCAAGAAATTCACCCCAGGTCCTCCTGCTCCGGGCTCAGCCCGGGCCCCCCACCGTCCTACCCACGAAGAACAGCAAACAATAGCACAGCCCCGGCCCTGGAAGCCGAGCTAGCCGCAGAGGGTGATGCGGAACTGCTCATCGCCAGGGAAACGCGGCATTCGGACAGCCGTCCAGAGCCTGTCTGAGTTCTGCTGGGCGGGGAGCCAGAGACTGATTCGCTGCGGTCCTGGGCCCAGCGGCTGGGAGGTCCCGCCCTGGCCGAGGAAATGAAGAGGGCGCAGGGCATGGGGGCGGCGCGCACCCGGGACAGTTTCCAGTCGGCTTCTCCGTGACCTTCCTCCCCCTTCCAGCTCGGAGCGGGATGCTGGGATCCCTCCAAGGACCCTCTGAGGACGTAGGAGGGATGCTGAGACAGGCAAGTGGGATTTCTCTACATATTGTCACCTCGCCCGCACTTTTGGAGAAGGTAGAGCCAAGCAGCATTTG', 'TGCTACAATGACAATCAGCAACATACAGTGAGGTTTTGATACATGTAAAAGGAAATTAATGTTTCTTCATTCTTATTTTGGGGCTTCTATTCTGTAAGTGGGGCCTGGGGAGAACAGGGACTATGAAGGTCCCTGTGGCAGTTCATTGCGTGGTTCCTATTAATATTAATATTAATGATACCACAGTGCCTTCACTTTTTTGAGTACGAAACTTGTTTATGACATTTTGTTGAAGACAAATGGAAGTTAAAAACAACAGCTTTTTCTTTTCCCATTAATTTTGTGTTTTTAATGGAGCAGGAACTGGGTTAGAGGCATAGTTTGTTTGAAGGGTATTTCTTCTCATGAAAGGGGTCAATGAAAAGCAAAAGCTTAGGAGATTTTGTGCCGGAGTATAACGGCAGGATAATAAATTAGTGATATTTTTAAAAAGATAGGTGAGAAAAGTCAGACTAATAGCCTGTTGCATAGTCTTTCAGATTCTACTTCTACCCCACACT', 'ACAAAGGATCACTTGATTAGAGGAGGAAGGCAGTATCACAAAGTAAACCGAAAAGACTTGCCCATTAAGACAGGCTATGAAGATGGGATGAAGGGCTTAGGGATAAAGAATTTTCTGTCCCTCTTTGGTCATGCTCTGGAGTGATACCTTGGGGCTATGCCATGAGCTGATAGGAGTCAACGCGAGTGGCTTTGCTGCCTCCTGGTTCCCTGTCTCCTTCAACTCCTTTCTTCCAGTAATCCAGTAAAAGGTGGTTGTCCTCCCTCTATTATAAAGGGTTTTCTCCACCAGCCAAGTAAATGAGGTCCAGTTGAAGAAGACAAGGCAGCACAATAGAAGACAGTAAGACCAGCCACCAAAAGCACTGGTTTTACATGGATTTGACTTTGATGGAATGAAGGGTCAGCTTTTCACTATCAACATGTGTCCACTTTGTCAGGAAGGGCCCCCTTGGGTGAGAGAACCCTGCTGCGCAGTAAGTGGGCTCCTGGTTTGAGCCC', 'ATGCTACCCTCTCCAGACCCTGAATCTGCTGGCATCTTGATCTTGCACTTCAGAGCCTCTGGTAACATAAGAAACAAATTTCTGTTGTTTATAAGGCACTCAGTCTATGGTATTTTGTTACAACAGCCCAAACGAACTAAGAAAAAAAGTTTTATTATTCTATTTTTTTATGAATTTAAGAAGAATTGTTATTAATTATTTGAATGTTTAGTAGAATTCACTTGAGAAGCTATCTAGTCCTGGGCTTTTCTTTGTTGGGAGGTTTTTAAATTCCTACTTCACTTTTTATTACTACCTGATTATTTAAATATTTATAGGTACTTAAAATCTTGTGTATGCAATATAAATGTAGCTGTACCTGCTTTCTCTGGGTAACATTTGCATGGAATATCTTTCTCTACCCCTGTATTTTCTACCTGTGTGCATCCTTAACTCTAAAGTGAGTTTCTTGTAGACAGCATTTTAAAAATTTATTAGCCACTCTGTCTTTTGATTCAAGA', 'CACCACACCCAGCTAATTTTTTGTATTTTTAGTAGAGATGGGGTTTCACTGTGTTAGCCAGGATGGTCTTGATCTCCTGACCTCATAAGCCAACCACCTCCGCCTCCCGAGATGATGTTTTGATAGCAAGACAAAAAAACTTGAATTAGGTAAACTATTTTCTCAATCTAGTGTGGTCTGGCTTTATAACAATATATATCAGTGAATTCTCTCTGTTAACAGAATTTGGTTTGCTGATGACACAGATGTGTGATATAACATTAACAAAGACATTTGGGAATTATTCTTGTTTTTAGTAATTTTGTAAGTTGATAGTGAGCACTGAAAGCTGTGGTCTGCACTTTGATATAATATTTTGCCATTCTATCTAGGAATAAACTCAGTCTTTTCTGACTGCGTTAGCACTGGTATGTTCATTTAAATTTAAGATGTTCTCAAATCAAACAAGCATCTAAACAGCAGTAAAATCCAACCATAAAAAGTGAATTCCCTGACTGACA', 'TTCTCTCTCACACACACACACACACACACACATATATACATATATATACGTGGCCAACTGCCTCGCCTCTAGCACTGGGAATCAGTCCCCGTGCTGTGCTTGTGGAGTCTTATAGCCCAGCAAGAGGAAGCTGTCTCCTGACATCGCCCCTCCAAAGTGCACCACCTCCAGTGAGCTTCCGGGACATGCACGGCCTGTGGACAGCCAGCCCCCGCCATCCCTCCCGCCCTTCTGGCCAAGCATGGCGGTGCTGTGCAGGCAGCTGTGTGGCCTGACAGTCTCTACCAGTCCTGCTGTCCCTCGGCTGAGAAACCCATTTCTGGATGACAGAGAATGTGTCCTCTGCTGGCTGTGTTCTCTATGGAGCTCAGGGGAGGGAAAAGGCCAAGCCATTTTTAGGGTGCTGTTGGGAGCAGTGAAAAGGTCACACCCTTTTCAAGGGACACTTTTCCTGGAAAGTCCCTGGAGCTTAGCTGGCTCTTACCCTGTGAAGCCGGCTC', 'GAGCCTGTCCATCCAAGTTGACATCTAAAAGACAGTTATGAATGTGGAGGCTGCAGGAAGACAGCAGGAACAGAGCTAGGTCCTGTCATTCTTCTGAACCACTACCCTAGGTTCCAGGACTCAAAAAGACAAAAGCTTGGAAGAACCTAAGCTCATAAAGCTACTCAGAGGCTATTTCTCCGGGTCTCCCCTAAGTCATATAGGCAAAGTACCTACCTCTGTGGACTCCCAGAGAAGTGGGTTCAATATTCTCAGAAACAGCTAGCTACTACCAGTAAATTCTTCCTTGCATATCTCTTCAACCACCCACATCATCTACACCTTTCTTCTAGATTTTGTGGTTGAAAAATGTTTGATGACAGCCAGTGGTATAATAGCCAAATACAGAGGAGTTATTTGGTCCAGACCAGGACTCAAACTTTAGAGACTGCAAAGGAATCTTTCTCAAGTTTCAGTCACTGAAGAGGAAACTGTGGTTGGGTTCCACCATCCACTCCACC', 'TAGTCTGAATTAGGGCCCACCCTACACCCTCATAGTAACTCACCTCTGTGAACGTTCTTTCTCTGAATACAGTCACATTCAAAGTACTGCGGGTTACAGCTTTAAGTTATGAATTTTGTGAGACACAATTCATCTCATAATGATATACATTATGTATGTGCATTACATGTTTAAGTATATTATAGGTATTCTAGGAAAGAGCCTATTTCCTATTTTTCAAAACATCTTTCTTATCCATCAGCAAAACACATGAATCACTTTAACTTTTGAGTCATTCTGAACTCTAATAGATAATAGGTGACTTATCATTTACAATTACCTTATGCCTCGGATTACAAAGTCATTCCTCACCTAGAGATTGTGTACAAGGCTCCTTTGAAGGCGATTAGGCAAAATATTTTCTTTCTATAAAAATACAACTCTTCTTACGTCAATGCGACTTGAAGAAAGGAGACATTGTCAGTTTTACAGGCTACAATAGAGCCTATTATCTGATTCAA', 'TGCATAGTGCAAGGGCGTCCTCCCTCTGGAATCTTTACAGAGAATTGACACTTCTTGTTCCAGCAGCATTTGCACACATTCCTCGTGTCCTGTCATAATCTGATGCATTGTAATTAAAAACACAAAACAAACCTCAGAAGACTAAGGAAGCAAAGCAGTTGGCTAAGTGAGGCTGGGCTTAGAGCAAGTGGACATATGCCTGGGTCTTTAAGGAAGGCAGACTCTGTCCTATTTCAAGGGCAGAAGGCAGGACAACCTCAGCAAAATTCATAACAGAAAAATCTGTGAACATCAGCATTTTCGAAATATCTCTTGCCTCTCCTGACAACTGTGAAAGATATCATGATGCGGCCGGGCGTGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCAGGAGGATCACTAGGTCAGGAGATCGAGACCATCCTGGCTAACATGGTGAAACCCCATCTCTACTAAAAAAAATACGAAAAATTAGCCTGGCATGGTC', 'TGCAATAAGATGGATAATGAAAAGATTCTTCTTCCACAAGCGTCGAGTGAGTCAGAGCTCTGTGCTTGACTTTGCCCCAGGGAGTACCGAGAAAATGAAATATCGTCTGTGTTCTCATGACACCTAAACAAATGGTCAAGTCCATGGGTCAAAAATCAGCCAGAGGGGTTAAGAGGCATCTCTAGCAAAGGAAGGGGAGGAGCAGCAAGGAAAGCACCATTAGCAAAGGTTGCTGTGTGGGGTCCTGCTACACGGCTGTTATTAAGGCTAATACCTGTTCCCCTTACAGTGAGATCTTTGTTTTGCCTTCACGAAACACACTGGTAAATGACAACAAGTCTGAAATCTGCAAGGATTTTGAACTTGTCAATCTGGCTCAATAATTTCTTCCACATCCTGACACCTGAATCAAGACATAAGTTCTCATCTAGGTAACAAACTATTTTCTTTCACAATGAGGATAAACCATCTCTGAATCTCAGATTTGCTCTTTTTTTTTC', 'ACACAATCAGACATAAAACAATCCTCAGCAAATGCAAAAGAACCAAAACCTTACCAACCACTCTCTCAGACCACAGCACAATAAAAACAGAATTTACAACTAAAGAAAATTGCTCAAAATCATACAATTACATGGAAATTAAAAAATAACCCTACTCCTGAATGACTTTTGAGTAAATAATGAGATTAGGGCAGAAATGAAGTTCTTTGAACCTAATGAGAACAAAGACACAACCTACCAAAATCTCTGGGACATGGCTAAGGCAGTGTTAAGAGGGAAATTTATAGCACTAAATATCCACATCAAAAAGTTAGAAAGATCTCAATTTAACAACCTAACCTCACAACTGAAAGAACTAGAGAGGCAAGAGCAAACATATCCCAAAGCCAACAGAAGACAAGAAATAACCAAAATCAGAGCTGAACTGAAGGTGACTGAGACATGAAAAACTACTGAAAGAATCAATGAAGCCAGAAGCTGATTTTTTTGAAAAAAAAAAA', 'TAATAGTGGGGACTAGGGGGTAGAGGGATGAAGGGGAGGTATAGTATCTACTTCTATAATGCTGAGATTTAAGAATATTTCCTTACCCGGTCCCTCAAGCCTATCATAAATATTCTATATTGGGTTGACAAATTACAGCTCACAGGCTAAATCTGGCCCTCTGCCTGTCGTGTAAATAAAATTTTGTTTGAACACAGCCATGCTCACTGGATTGTCTATGGCTGTTTGCACCTTGCAACAGCAGAGTTGAGTAGTTGCAACAGAGACCATGTGATCTGCAAAGCCTAAAATATTTACTATCTAGTCCTTTACAGAAAAAGGCTTTTGGCCTGTATTATTTAAACAAATCCATACGTGAGTTATACTGAGGACTCTCTACATGCTCAGTAGGCACCCAGTTTGTGCACATTGTAAATACACAATAGATTGGAAGCCCAGTTTCTATTCTCAAGGAATTTAAAATCTAGTTGAGGAGACAAAGACTAGCATACATGAAAAAA', 'GAACGGGACTGACTCAGGAGCCTGAGGACAGCAAGCGCCATCAACTGTGATGGCCAACAGCCACCAGACCGGGTCAGAGCAGGAAGCCAAAGGGCTGCCAGAGGAATGCCATCAGAAAGAATGAAATCAGACTCGAAACCAGAGAATGAAGGAGAAAGCAGCTGAGAAAACTGTGTGATACCAACAGGCATCATTCGGTTTCCTCTCAAGAGAATAAAAAGGACTGGGCAACAGGAAAAACTCAACTACAGGAAATCACAGAAGAAATGAAACATGCCCCCCGAGTGGTGTCCTCTGGGCCATCAGGGACGCCAGGAACAGCTCCCTTTGAGTGGGCACCTGGCTCTACGGTGAGTGGTATCTACATTCCTGTCATGGTGCTGGTCGCTGGTAGTTAACTTACAGAATCACCCTGTAGACAAAGCATGAAAGACCAACCCAGGCTGGAGAAGCAAAGCTATCTGTTACCAAGGTGAAGGAAGAGCTGACCAAAGTTTGAG', 'GAAGAATGCTAAGACTGCTGATTTCTGCATTTCCAACTGACGTACTGGGTTCATCTAATTGGGACTGGTTGGACAGTGGGTGCAGCCTACAGAGTGTGAGCCAAAGCAGGGCAGGGCATCGCCTCACCCAGAAAGCACAAGGGGTTGGGGAATTCCCCTTCCTAGCCAAGGGAAGCCATGACAGACAGTACCTGGAAAATCAGGACACTCCTGCCCTAATACTGCTCTTTTCCAACAGTCTTAGCAAACAGCACACCAGTAGATTATATCCCGCACCTGGCTTGGCAGGTCCCACGCCCGTGGAGCCTTGCTCACTGCTAGCACAGCAGTCTGAGGTCAAACTGCAAGGTGGCAGCGAGGCTGGGGGAGGGGCACCCGCCATTGCTGAGGCTTGAGTAGGTAAACAAACTGGCCAGGAAGCTCAAACTGGGTGGAGCCCACCTCAGCTCAAGGAGGCCTGCCTGCCTCTGTAGACTCCACCTCTGGGGGCAGGGCATAGG', 'GAAGAAAATGATAGGGCATTGAGCAAGTGACCTTCCTGTGTTTTGTTCACCTTCATTTATACTTTCTTTAGGATTTGACTGGAGAGAGGTGATCTCTTTTCTTCAGATACATCCCATAAGTTGTCCATAAAATGTGTCTGACCATGGTATCTTCCTTGGGAGGAGATTATAATTACTAATTCAATTATCTGTTGGTTATTGATATACTCTGGTTTTTCTATTTCTTCTTGATTCAATTTTAGAAATTTATGGTTACCTATAAAATAATCCATTGATCTAAGTTTCCAATTTATAGTCATTAAGTTATTGACCATGATTTTATAGCTTTTTAAAAATCTAGATTCTGTTTGCAATTATACCTTCATTTTTGTTCCTAATATCATTTATATGTGCCTTTGTTGTTATTCTCTTGAGTGATCTTGCCTGAATTTTAATATTGTTTTATTCTTTTTAAGGAACCTACTTTTATTTTGAGCATTTCTTATTGTTGTGCTTGTTAT', 'AAGCTCTACGAGCTGCACGCGGCCGGCGCCCCGCCCCCCCCGCCGCCCGGCCACGCCCCCGCGCCCGAGTCGCCGCGGCCCGGAAGCGGAAGCGGAAGCGGCCCCGGCCTCGCCCCTGCGCGCTCGCCCGGCCCCGCGGCCGACGGCCCCATCGACCTGAGCAAGAAGCCGCGGCGCCCGCTCCCCGGAGCCCCGGCACCGGCGCTGGCCGACTACCACGAGTGCACGGCCTGCCGCGTGAGCTTCCACAGCCTCGAGGCCTACCTGGCGCACAAGAAGTACTCGTGCCCCGCTGCGCCACCGCCCGGCGCGCTCGGCCTGCCCGCCGCCGCCTGCCCCTACTGCCCCCCGAACGGCCCGGTGCGCGGGGACCTGCTGGAGCATTTCCGCCTGGCGCACGGCCTGCTGCTCGGCGCGCCCCTGGCCGGCCCGGGGGTCGAGGCCCGGACGCCGGCCGACCGCGGCCCCTCGCCCGCTCCCGCCCCCGCCGCCTCCCCGCA', 'ATCCACCAAGCAGGTGTGTGCCTATGTCAAGAGCCAGAGGAACAGCTCCCAGTCCCAGGTGCCACTCTCTTTTGCTCATCTTATTCAATTTCTTTTTGGACTTTCTCATTTTATCTTCCATGTCGTTTACTCTTCTTCCATTTGTGCCATCTCATTGTTTCTCTGCATGGGCCCAAGGCCAAATTCTCTTTCTTCCTAAGTTCTGGAGCCTACAGCTGTGACCATTACATACCTGCCCACTCCCAGGACAGTTCACAAGCTTATAGCTCTGGTTTTTGGGATTCTTCTTAGTTTCACTTATCATAAGGTTTTCCTTTCTTTCAAGCTTAGTTATGAAATAATTACATATTATCTATTTTTTTTTTGAGATAGGAACTCACTATGCTGCCCAGGCAGGTCTTGAACTCCTGGTCTCAAGTGATCCTCCCACCTCAGCCTCCCAAGCAGCTGGAACATGAATGTGCACCACTGTACCTGGCTTCAATATTCTCATGGGTTTG', 'ACATGTACAGCATTATTTTTGCTCCCACTGAATTTGCTGTACAAACAACTGGCAATTGGGGTATGTTGGCTGAAATAATTTCATCACTTAATCCTCAATTTTACGTGTGACTGGTTGATTGCACACAGATCCTGTGACCCCTGAGTCTGTTTCCTAGTGATTGCCCGGGCTTATCACTATATTGTTTGCTTTCTGCTCTTCCCCGGGGTTTGCTTTCCAGGGGCAGTTGTGTGACCTGCCTGCCTCTTGGTGACCTCCTTTTCACTCTGGTGAACACTCCTTCATCTCCCCGTTTTCATTGATTTCTCGGGTATTTTCCATTCTGTAATACTTTGGCTGCTGTTTCTTCCTTGTTTTTATAAAATTCATCTGCTGGTTGCTGTTTACAAAACATCTCCTTTAAACCATCAGGCACGAAGGCAGGATTGCTGCCCCACTGGGAAGCCGGCCCCACAATCAGGTTCCACGTAAGGACGTTTTCCAGAAAATGACACGTGGCT')\n",
      "Labels: tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch number: {batch}\\nInputs: {x}\\nLabels: {y}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the transformation we just learned to convert the sequences into meaningful tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 0\n",
      "Inputs: tensor([[ 9, 10,  9,  ...,  9,  9,  1],\n",
      "        [ 8,  7,  9,  ...,  8, 10,  1],\n",
      "        [ 7,  7, 10,  ...,  8,  7,  1],\n",
      "        ...,\n",
      "        [ 7,  9, 10,  ..., 10,  8,  1],\n",
      "        [ 8,  8,  7,  ..., 10,  9,  1],\n",
      "        [ 8, 10,  9,  ...,  7,  7,  1]])\n",
      "Labels: tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch, (x, y) in enumerate(train_loader):\n",
    "    x = tokenizer(x, return_tensors='pt', padding=True, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "    print(f\"Batch number: {batch}\\nInputs: {x}\\nLabels: {y}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now you can see the relationship between the sequences and labels contained in the dataset. Let's now load the model you're going to train! It's really important to keep in mind that if you're going to use a pretrained model (as you are doing right now), you have to use the same `checkpoint` for both the tokenizer and the model. Doing so you will give the inputs to the model as it expects and avoid lots of error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at LongSafari/hyenadna-tiny-1k-seqlen-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model using the same checkpoint as the tokenizer\n",
    "# The num_labels parameter is set to 2 because we have two classes in the dataset (positive and negative)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, trust_remote_code=True, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are almost there to train your model! You're a few parameters and functions away of training the model. Let's finish the set up to start the training process!\n",
    "\n",
    "To start its training a deep learning model needs:\n",
    "* a `device`: where the math operations will occur. It's advised to use a GPU for accelerated training\n",
    "* `loss function`: this tells you how much difference is there between the model's precictions and the ground-truth labels in the dataset\n",
    "* a number of `epochs`: the amount of times the model will \"see\" the samples within the dataset\n",
    "* an `optimizer`: a way of calculating and update the model's parameters to fit better the data after each operation\n",
    "* a `training` and `test` loops: to declare how the information will pass through the network and in which moment to update the parameters\n",
    "\n",
    "Let's declare them in the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# create optimizer and define its parameters\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.1\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define a device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train(model, device, train_loader, max_length):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    size = len(train_loader.dataset)\n",
    "    for batch, (x, y) in enumerate(train_loader):\n",
    "        x = tokenizer(x, return_tensors='pt', padding=True, truncation=True, max_length=max_length)[\"input_ids\"] # Tokenize the input sequences\n",
    "        x, y = x.to(device), y.to(device) # Move the data to the device\n",
    "        # Forward pass\n",
    "        outputs = model(x, labels=y) # Get the outputs of the model\n",
    "        # Backward pass\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test loop\n",
    "def test(model, device, test_loader, max_length, losses, accuracies):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    size = len(test_loader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = tokenizer(x, return_tensors='pt', padding=True, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x, labels=y)\n",
    "            test_loss += outputs.loss.item()\n",
    "            correct += (outputs.logits.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    losses.append(test_loss)\n",
    "    correct /= size\n",
    "    accuracies.append(correct)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have all the parameters, objects, and functions needed to train a deep learning model! The last thing you have to do is to declare how many **epochs** the training phase will have. This is how many times will the model \"see\" or process all the samples contained in the training dataset. This is done to try to fit the model as most as possible to the data. That's why your dataset has to include a wide variety of examples, covering the most cases as possible from the phenomenon you're studying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.725337  [   32/20843]\n",
      "loss: 0.680516  [ 3232/20843]\n",
      "loss: 0.692339  [ 6432/20843]\n",
      "loss: 0.662142  [ 9632/20843]\n",
      "loss: 0.685585  [12832/20843]\n",
      "loss: 0.684949  [16032/20843]\n",
      "loss: 0.669596  [19232/20843]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.020826 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.687336  [   32/20843]\n",
      "loss: 0.661461  [ 3232/20843]\n",
      "loss: 0.691168  [ 6432/20843]\n",
      "loss: 0.648936  [ 9632/20843]\n",
      "loss: 0.639548  [12832/20843]\n",
      "loss: 0.634887  [16032/20843]\n",
      "loss: 0.622727  [19232/20843]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.020077 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.565129  [   32/20843]\n",
      "loss: 0.710092  [ 3232/20843]\n",
      "loss: 0.632840  [ 6432/20843]\n",
      "loss: 0.614384  [ 9632/20843]\n",
      "loss: 0.573174  [12832/20843]\n",
      "loss: 0.611180  [16032/20843]\n",
      "loss: 0.615772  [19232/20843]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.019404 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# Define the lists to store the losses and accuracies\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    train(model, device, train_loader, max_length)\n",
    "    test(model, device, test_loader, max_length, losses, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Dive into Deep Learning\n",
    "2. Genomic Benchmarks paper and repository\n",
    "3. HuggingFace"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsc_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
